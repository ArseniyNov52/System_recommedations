import logging
import pandas as pd
from typing import List, Dict
from sentence_transformers import SentenceTransformer, util
from database.database import load_recommendation_history, save_recommendation_history
from ai_tools.analyze_system import extract_tags_and_genres
from ai_tools.summarize_system import summarize_text_and_update_db as compress_text
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np


logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('recommendation_debug.log')
    ]
)
logger = logging.getLogger(__name__)

similarity_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")


def log_call(func):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤—ã–∑–æ–≤–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π"""

    def wrapper(*args, **kwargs):
        logger.debug(f"–í—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏ {func.__name__} —Å args={args}, kwargs={kwargs}")
        try:
            result = func(*args, **kwargs)
            logger.debug(f"–§—É–Ω–∫—Ü–∏—è {func.__name__} –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å —É—Å–ø–µ—à–Ω–æ")
            return result
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ñ—É–Ω–∫—Ü–∏–∏ {func.__name__}: {str(e)}", exc_info=True)
            raise

    return wrapper

def preprocess_text(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    if not isinstance(text, str):
        return ""
    text = text.lower()
    text = re.sub(r'[^–∞-—è—ë\s]', '', text)
    stopwords = {"—ç—Ç–æ", "–∫–∞–∫", "—á—Ç–æ", "–∫–Ω–∏–≥–∞", "—Ä–æ–º–∞–Ω"}
    return " ".join([word for word in text.split() if word not in stopwords])


@log_call
def find_related_books(query: str, dataset: pd.DataFrame, count: int) -> List[Dict]:
    """–ù–∞—Ö–æ–¥–∏—Ç —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""

    query_lower = query.lower()
    category_matches = dataset["category"].str.lower().str.contains(query_lower, na=False)
    desc_matches = dataset["description"].str.lower().str.contains(query_lower, na=False)

    combined_matches = dataset[category_matches | desc_matches]


    if len(combined_matches) < count:
        remaining = count - len(combined_matches)
        random_sample = dataset.sample(min(len(dataset), remaining))
        combined_matches = pd.concat([combined_matches, random_sample])

    return combined_matches.head(count).to_dict("records")


def recommend_books_by_genres_or_text(user_text, books_dataset):
    """–î–æ–±–∞–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –∂–∞–Ω—Ä–∞–º"""
    user_genres = extract_tags_and_genres(user_text)
    genre_candidates = []

    for book in books_dataset:
        book_genres = book.get("genres", [])

        if book_genres:
            user_genre_embeddings = similarity_model.encode(user_genres, convert_to_tensor=True)
            book_genre_embeddings = similarity_model.encode(book_genres, convert_to_tensor=True)
            cosine_scores = util.cos_sim(user_genre_embeddings, book_genre_embeddings)

            if cosine_scores.max().item() > 0.6:
                genre_candidates.append(book)

    if genre_candidates:
        book_texts = [b["description"] for b in genre_candidates]
        similarities = get_semantic_similarities(user_text, book_texts)
        sorted_books = [book for _, book in
                        sorted(zip(similarities, genre_candidates), key=lambda x: x[0], reverse=True)]
        return [{"name": b["name"], "author": b["author"], "description": b["description"]}
                for b in sorted_books[:5]]
    else:
        all_texts = [b["description"] for b in books_dataset]
        similarities = get_semantic_similarities(user_text, all_texts)
        sorted_books = [book for _, book in sorted(zip(similarities, books_dataset), key=lambda x: x[0], reverse=True)]
        return [{"name": b["name"], "author": b["author"], "description": b["description"]}
                for b in sorted_books[:5]]


def get_semantic_similarities(user_text, texts):
    """–î–æ–±–∞–≤–ª–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
    user_embedding = similarity_model.encode(user_text, convert_to_tensor=True)
    book_embeddings = similarity_model.encode(texts, convert_to_tensor=True)
    cosine_scores = util.cos_sim(user_embedding, book_embeddings)[0]
    return cosine_scores.cpu().tolist()


@log_call
def format_books(book_list):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–Ω–∏–≥ –≤ –∫—Ä–∞—Å–∏–≤—ã–π –≤–∏–¥ —Å –æ–ø–∏—Å–∞–Ω–∏—è–º–∏ –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è–º–∏."""
    logger.debug(f"–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {len(book_list)} –∫–Ω–∏–≥")

    if not book_list:
        return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π üòî"

    formatted_books = ["‚ú® üìö *–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∫–Ω–∏–≥–∏* üìö ‚ú®\n"]

    for i, book in enumerate(book_list, 1):
        title = book.get("name", "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è")
        authors = book.get("author", "–ë–µ–∑ –∞–≤—Ç–æ—Ä–∞").replace("By ", "").strip()
        description = book.get("description", "–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

        short_description = (description[:250] + '...') if len(description) > 250 else description

        formatted_books.append(
            f"üîπ *{title}*\n"
            f"üë§ *–ê–≤—Ç–æ—Ä:* {authors}\n"
            f"üìú *–û–ø–∏—Å–∞–Ω–∏–µ:* {short_description}\n"
        )

        if i < len(book_list):
            formatted_books.append("\n‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n")

    return "".join(formatted_books)


@log_call
def compute_similarity(target_text: str, books_dataset: pd.DataFrame, top_n: int = 5) -> pd.DataFrame:
    """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–∞–∑–º–µ—Ä–æ–≤"""
    try:
        if books_dataset.empty:
            logger.warning("–ü—É—Å—Ç–æ–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –≤—Ö–æ–¥–µ")
            return pd.DataFrame(columns=["name", "author", "description", "similarity"])


        target_clean = preprocess_text(target_text)
        books_dataset["clean_text"] = books_dataset["description"].fillna("").apply(preprocess_text)


        if books_dataset["clean_text"].empty or not target_clean:
            logger.error("–ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            return books_dataset.head(top_n).assign(similarity=0.0)


        target_embedding = similarity_model.encode(target_clean, convert_to_tensor=True)
        book_embeddings = similarity_model.encode(books_dataset["clean_text"].tolist(), convert_to_tensor=True)

        similarities = util.pytorch_cos_sim(target_embedding, book_embeddings)[0].cpu().numpy()


        result_df = books_dataset.copy()
        result_df["similarity"] = similarities

        return result_df.nlargest(top_n, "similarity")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ compute_similarity: {str(e)}")
        return books_dataset.head(top_n).assign(similarity=0.0)


@log_call
async def get_preferences_from_input(user_input: str) -> Dict[str, List[str]]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–≥–∏ –∏ –∂–∞–Ω—Ä—ã –∏–∑ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞."""
    logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –∏–∑ –≤–≤–æ–¥–∞: '{user_input[:50]}...'")
    extracted_data = await extract_tags_and_genres(user_input)
    logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {extracted_data}")
    return extracted_data


@log_call
async def get_preferences_from_history(selected_history: str) -> Dict[str, List[str]]:
    """–°–∂–∏–º–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –∫–Ω–∏–≥–∏ –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–≥–∏/–∂–∞–Ω—Ä—ã."""
    logger.debug(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª–∏–Ω–æ–π {len(selected_history)} —Å–∏–º–≤–æ–ª–æ–≤")

    try:
        compressed_text = selected_history
        try:
            compressed_text = await compress_text(
                selected_history,
                book_id=0,
                overall_target_chars=1000
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–∂–∞—Ç–∏—è —Ç–µ–∫—Å—Ç–∞: {str(e)}")

        extracted_data = await extract_tags_and_genres(compressed_text)
        logger.debug(f"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {extracted_data}")
        return {
            "tags": list(set(extracted_data.get("tags", []))),
            "genres": list(set(extracted_data.get("genres", []))),
        }, compressed_text

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∏—Å—Ç–æ—Ä–∏–∏: {str(e)}")
        try:
            extracted_data = await extract_tags_and_genres(selected_history)
            return {
                "tags": list(set(extracted_data.get("tags", []))),
                "genres": list(set(extracted_data.get("genres", []))),
            }, selected_history
        except Exception as fallback_error:
            logger.error(f"–î–∞–∂–µ fallback –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {str(fallback_error)}")
            return {"tags": [], "genres": []}, selected_history


@log_call
def combine_preferences(pref1: Dict[str, List[str]], pref2: Dict[str, List[str]]) -> Dict[str, List[str]]:
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç–µ–≥–∏ –∏ –∂–∞–Ω—Ä—ã."""
    combined = {
        "tags": list(set(pref1.get("tags", []) + pref2.get("tags", []))),
        "genres": list(set(pref1.get("genres", []) + pref2.get("genres", []))),
    }
    logger.debug(f"–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è: {combined}")
    return combined


@log_call
def filter_books_by_tags(dataset, preferences: List[str], max_candidates: int = 40):
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –∫–Ω–∏–≥–∏ –ø–æ —Ç–µ–≥–∞–º –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–∏."""
    logger.debug(f"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–µ–≥–∞–º: {preferences}")

    if not preferences:
        return dataset.sample(min(len(dataset), max_candidates))

    preferences = [tag.lower() for tag in preferences if tag.lower() != "—Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π"]

    category_matches = dataset["category"].str.lower().isin(preferences)

    if not category_matches.any():
        description_matches = dataset["description"].str.lower().str.contains('|'.join(preferences))
        filtered = dataset[description_matches]
    else:
        filtered = dataset[category_matches]

    if len(filtered) == 0:
        return dataset.sample(min(len(dataset), max_candidates))

    return filtered.sample(min(len(filtered), max_candidates))


@log_call
async def search_books_1_mode(selected_history: str, dataset: pd.DataFrame) -> List[Dict]:
    try:

        pref_history, compressed_text = await get_preferences_from_history(selected_history)
        preferences = pref_history["tags"] + pref_history["genres"]


        filtered_books = filter_books_by_tags(dataset, preferences)


        recommendations = compute_similarity(
            target_text=compressed_text,
            books_dataset=filtered_books,
            top_n=5
        )


        if recommendations.empty:
            logger.warning("–ù–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π. –í–æ–∑–≤—Ä–∞—â–∞—é —Å–ª—É—á–∞–π–Ω—ã–µ –∫–Ω–∏–≥–∏")
            return dataset.sample(min(5, len(dataset)))[["name", "author", "description"]].to_dict("records")

        return recommendations[["name", "author", "description"]].to_dict("records")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ search_books_1_mode: {str(e)}")
        return dataset.sample(min(5, len(dataset)))[["name", "author", "description"]].to_dict("records")


@log_call
async def search_books_2_mode(user_input: str, dataset: pd.DataFrame) -> List[Dict]:
    try:

        pref_input = await get_preferences_from_input(user_input)
        preferences = pref_input["tags"] + pref_input["genres"]


        filtered_books = filter_books_by_tags(dataset, preferences, max_candidates=100)


        recommendations = compute_similarity(
            target_text=user_input,
            books_dataset=filtered_books,
            top_n=5
        )


        if not recommendations.empty and recommendations.iloc[0]["similarity"] < 0.3:
            logger.warning("–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π. –í–æ–∑–≤—Ä–∞—â–∞—é –∫–Ω–∏–≥–∏ –ø–æ —Ç–µ–≥–∞–º")
            return filtered_books[["name", "author", "description"]].head(5).to_dict("records")

        return recommendations[["name", "author", "description"]].to_dict("records")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ search_books_2_mode: {str(e)}")
        return dataset.sample(min(5, len(dataset)))[["name", "author", "description"]].to_dict("records")@log_call
async def search_books_3_mode(user_input: str, selected_history: str, dataset: pd.DataFrame) -> List[Dict]:
    try:

        pref_input = await get_preferences_from_input(user_input)
        pref_history, compressed_text = await get_preferences_from_history(selected_history)
        combined_pref = combine_preferences(pref_input, pref_history)


        filtered_books = filter_books_by_tags(
            dataset,
            combined_pref["tags"] + combined_pref["genres"],
            max_candidates=150
        )


        combined_text = f"{user_input} {compressed_text}"


        recommendations = compute_similarity(
            target_text=combined_text,
            books_dataset=filtered_books,
            top_n=5
        )


        if len(recommendations) < 3:
            extra_books = filter_books_by_tags(dataset, pref_input["tags"], 2)
            recommendations = pd.concat([recommendations, extra_books])

        return recommendations[["name", "author", "description"]].to_dict("records")

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ search_books_3_mode: {str(e)}")
        return dataset.sample(min(5, len(dataset)))[["name", "author", "description"]].to_dict("records")

@log_call
async def get_book_recommendations(
        user_input: str,
        selected_book: str,
        mode: int,
        user_id: str,
        history_exclude_option: bool,
        dataset
) -> str:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
    logger.info("=" * 50)
    logger.info(f"–ù–ê–ß–ê–õ–û –û–ë–†–ê–ë–û–¢–ö–ò –ó–ê–ü–†–û–°–ê (—Ä–µ–∂–∏–º {mode})")
    logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—ã–∑–æ–≤–∞:")
    logger.info(f"- user_input: {user_input[:100]}...")
    logger.info(f"- selected_book: {selected_book[:100] if selected_book else 'None'}")
    logger.info(f"- user_id: {user_id}")
    logger.info(f"- history_exclude_option: {history_exclude_option}")
    logger.info(f"- dataset –∫–æ–ª-–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(dataset)}")

    if history_exclude_option:
        logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")
        previously_recommended = load_recommendation_history(user_id)
        logger.info(f"–†–∞–Ω–µ–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ –∫–Ω–∏–≥–∏: {previously_recommended}")
        dataset = dataset[~dataset["name"].isin(previously_recommended)]
        logger.info(f"–û—Å—Ç–∞–ª–æ—Å—å –∫–Ω–∏–≥ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(dataset)}")

    try:
        if mode == 1:
            recommendations = await search_books_1_mode(selected_book, dataset)
        elif mode == 2:
            recommendations = await search_books_2_mode(user_input, dataset)
        elif mode == 3:
            recommendations = await search_books_3_mode(user_input, selected_book, dataset)
        else:
            raise ValueError(f"–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π —Ä–µ–∂–∏–º: {mode}")

        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {len(recommendations)}")
        save_recommendation_history(user_id, [book["name"] for book in recommendations])
        return format_books(recommendations)

    except Exception as e:
        logger.error(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {str(e)}", exc_info=True)
        raise